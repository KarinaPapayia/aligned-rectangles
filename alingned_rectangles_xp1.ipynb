{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dset\n",
    "from torchvision import transforms\n",
    "from configuration_generator import RectangleConfigurationGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTRS = dict(min_w=10, max_w=50, min_h=10, max_h=50, min_x0 = 0, min_y0 = 0, padding = 5, canvas_size = 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = RectangleConfigurationGenerator()\n",
    "b, d = gen.generate_training_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training samples:\n",
    "training_dataset = gen.generate_training_data(200)\n",
    "#for _ in training_dataset:\n",
    " #   print(b[0], d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert boolean data to bitmap data for the aligned and the corrupted rectnagles\n",
    "\n",
    "#transform Image to np.array\n",
    "def boolstr_to_floatstr(v):\n",
    "    if v == 'True':\n",
    "        return '1'\n",
    "    elif v == 'False':\n",
    "        return '0'\n",
    "    else:\n",
    "        return v\n",
    "\n",
    "def preprocess(sample):\n",
    "    #vectorization to get 0s and 1s\n",
    "    new_data = np.vectorize(boolstr_to_floatstr)(sample).astype(float)\n",
    "    data = torch.Tensor(list(new_data))\n",
    "    # resize:\n",
    "    return data.view(1, 28, 28)\n",
    "\n",
    "\n",
    "beautified = (b for b, *_ in training_dataset)\n",
    "distorted = (d for _, d, *_ in training_dataset)\n",
    "beautified = list(map(preprocess, beautified))\n",
    "distorted = list(map(preprocess, distorted))\n",
    "beautified_img = torch.stack([torch.Tensor(i) for i in beautified])\n",
    "distorted_img = torch.stack([torch.Tensor(i) for i in distorted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Hyperparameters\n",
    "\n",
    "epoch = 200\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "#data processing\n",
    "#to-do add dataset\n",
    "dataset = beautified_img\n",
    "\n",
    "#set data loader(input pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset = dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalization(tensor, min_value, max_value):\n",
    "    min_tensor = tensor.min()\n",
    "    tensor = (tensor - min_tensor)\n",
    "    max_tensor = tensor.max()\n",
    "    tensor = tensor / max_tensor\n",
    "    tensor = tensor * (max_value - min_value) + min_value\n",
    "    \n",
    "def tensor_round(tensor):\n",
    "    return torch.round(tensor)\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda tensor: min_max_normalization(tensor, 0, 1)),\n",
    "    transforms.Lambda(lambda tensor: tensor_round(tensor))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder \n",
    "# torch.nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "#                 stride = 1, padding = 0, dilation = 1,\n",
    "#                 groups = 1, bias = True)\n",
    "# batch x 1 x 28 x 28 -> batch x 512\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                        nn.Conv2d(1,32,3,padding = 1),   # batch x 16 x 28 x 28\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(32),\n",
    "                        nn.Conv2d(32,32,3,padding = 1),   # batch x 16 x 28 x 28\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(32),\n",
    "                        nn.Conv2d(32,64,3,padding = 1),  # batch x 32 x 28 x 28\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.Conv2d(64,64,3,padding = 1),  # batch x 32 x 28 x 28\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.MaxPool2d(2,2)   # batch x 64 x 14 x 14\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "                        nn.Conv2d(64,128,3,padding = 1),  # batch x 64 x 14 x 14\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(128),\n",
    "                        nn.Conv2d(128,128,3,padding = 1),  # batch x 64 x 14 x 14\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(128),\n",
    "                        nn.MaxPool2d(2,2),\n",
    "                        nn.Conv2d(128,256,3,padding = 1),  # batch x 64 x 7 x 7\n",
    "                        nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(batch_size, -1)\n",
    "        return out\n",
    "    \n",
    "encoder = Encoder()\n",
    "\n",
    "# Decoder \n",
    "# torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size,\n",
    "#                          stride=1, padding=0, output_padding=0,\n",
    "#                          groups=1, bias=True)\n",
    "#output_height = (height-1)*stride + kernel_size - 2*padding + output_padding\n",
    "#output_size = (input_size + 2*padding - kernel_size)/stride + 1 \n",
    "# batch x 512 -> batch x 1 x 28 x 28\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(256,128,3,2,1,1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(128),\n",
    "                        nn.ConvTranspose2d(128,128,3,1,1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(128),\n",
    "                        nn.ConvTranspose2d(128,64,3,1,1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(64),\n",
    "                        nn.ConvTranspose2d(64,64,3,1,1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(64)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(64,32,3,1,1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(32),\n",
    "                        nn.ConvTranspose2d(32,32,3,1,1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(32),\n",
    "                        nn.ConvTranspose2d(32,1,3,2,1,1),\n",
    "                        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = x.view(batch_size, 256, 7, 7)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        return out\n",
    "\n",
    "decoder = Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check output of autoencoder\n",
    "for image in train_loader:\n",
    "    image = Variable(image)\n",
    "    output = encoder(image)\n",
    "    output = decoder(output)\n",
    "    print(output.size())\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss func and optimizer\n",
    "# we compute reconstruction after decoder so use Mean Squared Error\n",
    "# In order to use multi parameters with one optimizer,\n",
    "# concat parameters after changing into list\n",
    "\n",
    "parameters = list(encoder.parameters())+ list(decoder.parameters())\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(parameters, lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train encoder, decoder\n",
    "# save and load model\n",
    "\n",
    "#try:\n",
    "#    encoder, decoder = torch.load('./model/denoising_autoencoder.pkl')\n",
    "#    print(\"\\n--------model restored--------\\n\")\n",
    "#except:\n",
    "#    print(\"\\n--------model not restored--------\\n\")\n",
    "#    pass\n",
    "\n",
    "for i in range(epoch):\n",
    "    for image in train_loader:\n",
    "        #d_image = distorted_img\n",
    "        image = Variable(image)\n",
    "        #d_image = Variable(d_image)\n",
    "        optimizer.zero_grad()\n",
    "        output = encoder(image)\n",
    "        output = decoder(output)\n",
    "        #plt.imshow(out[0], cmap = 'gray')\n",
    "        #plt.show()\n",
    "        loss = loss_func(output, image)\n",
    "        loss.backward()\n",
    "        optimizer.step()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check output with corrupted image as input\n",
    "\n",
    "for i in np.arange(85, 99, 1):\n",
    "    img = beautified_img[i]\n",
    "    #input_img = distorted_img[80]\n",
    "    input_img = beautified_img[i] #to learn the identity\n",
    "    output_img = output[i]\n",
    "\n",
    "    #origin = img.data.numpy()\n",
    "    inp = input_img.data.numpy()\n",
    "    out = output_img.data.numpy()\n",
    "    \n",
    "    #plt.imshow(img[0], cmap = 'gray')\n",
    "    #plt.show()\n",
    "    \n",
    "    plt.imshow(inp[0], cmap = 'gray')\n",
    "    plt.show()\n",
    "\n",
    "    plt.imshow(out[0], cmap = 'gray')\n",
    "    plt.show()\n",
    "    np.shape(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = []\n",
    "out = []\n",
    "for i in np.arange(85, 99, 1):\n",
    "    img = beautified_img[i]\n",
    "    #input_img = distorted_img[80]\n",
    "    input_img = beautified_img[i] #to learn the identity\n",
    "    output_img = output[i]\n",
    "\n",
    "    #origin = img.data.numpy()\n",
    "    inp.append(input_img.data.numpy())\n",
    "    out.append(output_img.data.numpy())\n",
    "print(np.shape(inp[10][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(images, cols = 1, titles = None):\n",
    "    \"\"\"Display a list of images\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    images: List of np.arrays compatible with plt.imshow.\n",
    "    \n",
    "    cols: Number of columns in figure (number of rows is \n",
    "                        set to np.ceil(n_images/float(cols))).\n",
    "    \n",
    "    titles: List of titles corresponding to each image. Must have\n",
    "            the same length as titles.\n",
    "    \"\"\"\n",
    "    assert((titles is None)or (len(images) == len(titles)))\n",
    "    n_images = len(images)\n",
    "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "    fig = plt.figure()\n",
    "    for n, (image, title) in enumerate(zip(images, titles)):\n",
    "        a = fig.add_subplot(cols, np.ceil(n_images / float(cols)), n + 1)\n",
    "        if image.ndim == 2:\n",
    "            plt.gray()\n",
    "        plt.imshow(image[0])\n",
    "        a.set_title(title)\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n",
    "    plt.show()\n",
    "show_images(inp, cols = 1, titles = None)\n",
    "show_images(out, cols = 1, titles = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
